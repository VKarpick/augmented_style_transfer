{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AugmentTransfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtujCSm5XMtC+2/eWjwHIG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VKarpick/augmented_style_transfer/blob/main/AugmentTransfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifqPWq5BJlGr"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jinAqihgI6he"
      },
      "source": [
        "This workbook is for pairing image segmentation with neural style transfer.  It allows for different styles to be applied to different elements within images.\n",
        "\n",
        "The style transfer work comes predominantly from [here](https://nextjournal.com/gkoehler/pytorch-neural-style-transfer) while the segmentation work is courtesy [this link](\n",
        "https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/).\n",
        "\n",
        "If you intend to use it, make sure to use the GPU (Runtime -> Change runtime type)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7q8-fPmxd9C"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import models, transforms\n",
        "import torch\n",
        "from google.colab import files\n",
        "from io import BytesIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sivbq0G3xm8T"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf8Wiyp9KEM3"
      },
      "source": [
        "# Pre-trained Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQW6JSZHKK06"
      },
      "source": [
        "Deeplabv3_ResNet101 is used for the image segmentation and VGG19 for style transfer.  The categories Deeplab allows for and the colors chosen to represent them are as follows:\n",
        "\n",
        "*   background = not quite black\n",
        "*   aeroplane = green\n",
        "*   bicycle = lime\n",
        "*   bird = cyan\n",
        "*   boat = navy\n",
        "*   bottle = blue\n",
        "*   bus = yellow\n",
        "*   car = red\n",
        "*   cat = orange\n",
        "*   chair = beige\n",
        "*   cow = grey\n",
        "*   diningtable = maroon\n",
        "*   dog = teal\n",
        "*   horse = brown\n",
        "*   motorbike = pink\n",
        "*   person = purple\n",
        "*   pottedplant = olive\n",
        "*   sheep = white\n",
        "*   sofa = apricot\n",
        "*   train = mint\n",
        "*   tvmoniter = lavender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlwLR1Bzxn7P"
      },
      "source": [
        "dlab = models.segmentation.deeplabv3_resnet101(pretrained=True).to(device).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu1zX1Lfxp51"
      },
      "source": [
        "vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "# convert max pooling layers to average pooling\n",
        "for name, layer in vgg.named_children():\n",
        "  if isinstance(layer, torch.nn.MaxPool2d):\n",
        "    vgg[int(name)] = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "# freeze it so no gradients are computed\n",
        "for param in vgg.parameters():\n",
        "  param.requires_grad_(False)\n",
        "    \n",
        "# move to gpu\n",
        "vgg.to(device).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc2nx6spLiOi"
      },
      "source": [
        "# Image Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svSkbB-vLn1x"
      },
      "source": [
        "These are all the functions required to manipulate the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF14wVBpeYAM"
      },
      "source": [
        "def open_image(image_file):\n",
        "  \"\"\"Finds and opens the first uploaded image.\"\"\"\n",
        "  \n",
        "  return PIL.Image.open(BytesIO(list(image_file.values())[0])).convert(\"RGB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SOvyV-W0M8j"
      },
      "source": [
        "def transform_image(image, shape=(224, 224)):\n",
        "  \"\"\"Converts an image into a tensor of the given shape.\"\"\"\n",
        "\n",
        "  if shape == (0, 0):\n",
        "    shape = (image.height, image.width)\n",
        "\n",
        "  image_transforms = transforms.Compose([\n",
        "    transforms.Resize(shape),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                          std=[0.229, 0.224, 0.225])\n",
        "  ])\n",
        "\n",
        "  return image_transforms(image)[:3, :, :].unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DKEMOXK1KnA"
      },
      "source": [
        "def tensor_to_np(tensor):\n",
        "  \"\"\"Reverses a tensor of an image into a numpy array that matplotlib can display.\"\"\"\n",
        "  \n",
        "  image = tensor.to(\"cpu\").clone().detach()\n",
        "  image = image.numpy().squeeze(axis=0)\n",
        "  image = image.transpose(1, 2, 0)\n",
        "  image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "  image = image.clip(0, 1)    # color data from 0-1 rather than 0-255\n",
        "\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5NtORI93Wsw"
      },
      "source": [
        "def tensor_to_pil(tensor):\n",
        "  \"\"\"Converts a tensor to a PIL image\"\"\"\n",
        "\n",
        "  #PIL doesn't support float range from images, need to convert from 0-1 to 0-255\n",
        "  return PIL.Image.fromarray((tensor_to_np(tensor) * 255).astype(np.uint8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJn1H_JQ4Bpx"
      },
      "source": [
        "def label_to_color(image, label_nos=list(range(21))):\n",
        "  \"\"\"Converts colors of the image based on segmentation.\"\"\"\n",
        "\n",
        "  label_colors = np.array(\n",
        "      [(1, 1, 1),  # 0) background = not quite black\n",
        "       (60, 180, 75),  # 1) aeroplane = green\n",
        "       (210, 245, 60), # 2) bicycle = lime\n",
        "       (70, 240, 240),  # 3) bird = cyan\n",
        "       (0, 0, 128),  # 4) boat = navy\n",
        "       (0, 130, 200),  # 5) bottle = blue\n",
        "       (255, 225, 25),  # 6) bus = yellow\n",
        "       (230, 25, 75),  # 7) car = red\n",
        "       (245, 130, 48),  # 8) cat = orange\n",
        "       (255, 250, 200),  # 9) chair = beige\n",
        "       (128, 128, 128),  # 10) cow = grey\n",
        "       (128, 0, 0),  # 11) diningtable = maroon\n",
        "       (0, 128, 128),  # 12) dog = teal\n",
        "       (170, 110, 40),  # 13) horse = brown\n",
        "       (250, 190, 212),  # 14) motorbike = pink\n",
        "       (145, 30, 180),  # 15) person = purple\n",
        "       (128, 128, 0),  # 16) pottedplant = olive\n",
        "       (255, 255, 255),  # 17) sheep = white\n",
        "       (255, 215, 180),  # 18) sofa = apricot\n",
        "       (170, 255, 195),  # 19) train = mint\n",
        "       (220, 190, 255),  # 20) tvmoniter = lavender\n",
        "      ])\n",
        "  \n",
        "  r = torch.zeros_like(image)\n",
        "  g = torch.zeros_like(image)\n",
        "  b = torch.zeros_like(image)\n",
        "\n",
        "  for label_no in label_nos:\n",
        "    idx = image == label_no\n",
        "    r[idx] = label_colors[label_no, 0]\n",
        "    g[idx] = label_colors[label_no, 1]\n",
        "    b[idx] = label_colors[label_no, 2]\n",
        "\n",
        "  rgb = torch.stack([r, g, b], axis=0)\n",
        "\n",
        "  return rgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuoGERoX7wgM"
      },
      "source": [
        "def segment(image, label_nos=list(range(21))):\n",
        "  \"\"\"Converts an image into the segmented representation of the image.\"\"\"\n",
        "  \n",
        "  segmented_image = dlab(image)[\"out\"]\n",
        "  segmented_image = torch.argmax(segmented_image.squeeze(), dim=0)\n",
        "  segmented_image = label_to_color(segmented_image, label_nos)\n",
        "  segmented_image = torch.div(segmented_image, 255.0)\n",
        "  segmented_image = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                                         std=[0.229, 0.224, 0.225])(segmented_image)\n",
        "  segmented_image = segmented_image.unsqueeze(0)\n",
        "\n",
        "  return segmented_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFn542KeOLhj"
      },
      "source": [
        "def convert_to_original_colors(image):\n",
        "  \"\"\"Converts an images colors to remain consistent with the content image.\"\"\"\n",
        "\n",
        "  content_channels = list(tensor_to_pil(content_image).convert('YCbCr').split())\n",
        "  image_channels = list(image.convert('YCbCr').split())\n",
        "  content_channels[0] = image_channels[0]\n",
        "  converted_image = PIL.Image.merge('YCbCr', content_channels).convert('RGB')\n",
        "  return transform_image(converted_image, content_image.shape[2:]).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W6mVw5qJtxN"
      },
      "source": [
        "def update_final_image(object_image, background_image):\n",
        "  \"\"\"Combines two copies of an image, one with the updated objects and one with the updated background.\"\"\"\n",
        "\n",
        "  segmented_image = content_segment\n",
        "  \n",
        "  # de-normalize segmented image to set black to (0, 0, 0)\n",
        "  segmented_image = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "                                         std=[1/0.229, 1/0.224, 1/0.225])(content_segment.squeeze(0))\n",
        "\n",
        "  # find sections of the segmented content image that aren't black\n",
        "  # black = background to be update, anything else = object to be updated\n",
        "  where_segment = (segmented_image > 0)\n",
        "\n",
        "  # combine the images\n",
        "  image = object_image.squeeze(0) * where_segment + background_image.squeeze(0) * (torch.logical_not(where_segment))\n",
        "  \n",
        "  return image.unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5iKDXz2OhA9"
      },
      "source": [
        "# Image Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh40ksFTOlwc"
      },
      "source": [
        "For all of these, if multiple images are selected at once, only one will actually be used.  To change images, these cells will need to be re-run.\n",
        "\n",
        "All but the segmented image are required, even if one of the style images isn't going to be used.\n",
        "\n",
        "The first upload is the content image.  This is the image that the segmentation and style transfer will be applied to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwh-I5SJx2TS"
      },
      "source": [
        "content_image_file = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lvLCELHkWof"
      },
      "source": [
        "The next upload is for when Deeplab doesn't segment the image as desired.  It allows for an upload to be uploaded for use as the segmented image.  To use it, click the checkbox and upload the image.\n",
        "\n",
        "For creating a segmented image, any portions of the content image that aren't to be considered objects should be blacked out.  Everything else can be left as is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvQUqsDIkHgV"
      },
      "source": [
        "use_custom_segment = True #@param {type: \"boolean\"}\n",
        "\n",
        "if use_custom_segment:\n",
        "  custom_segment_file = files.upload()\n",
        "else:\n",
        "  custom_segment_file = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNROwtq_PByd"
      },
      "source": [
        "The second upload is the object style image.  This is the image containing the style that will be applied to any objects found in the segmentation.  For example, when taking an image of a person in which one style will be applied to the person and another to the background, this is the image whose style will be applied to the person."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvLvGozrydff"
      },
      "source": [
        "object_style_image_file = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6nR2msAPcFQ"
      },
      "source": [
        "The final upload is the background style image.  This is the image containing the style that will be applied to the background."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl8MLiVc3b9B"
      },
      "source": [
        "background_style_image_file = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AviZUYhumroD"
      },
      "source": [
        "# Image Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-A05bGNQa_W"
      },
      "source": [
        "All images will be reshaped to the same size.  The default has been set at 224.  It's generally recommended to use larger dimensions as they tend to work better for style transfer, but keep in mind that sizes that are too large will overtax the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeqmNodmRdPM"
      },
      "source": [
        "height =  277#@param {type: \"number\"}\n",
        "width =  474#@param {type: \"number\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tofa09oiRfu7"
      },
      "source": [
        "These checkboxes decide which objects will be detected by the segmentation.  Any category selected will have the same style applied to it.\n",
        "\n",
        "If applying style transfer to the entire image, all can be checked and the weights (in a later section) will have to be set accordingly.\n",
        "\n",
        "If the desired effect is for different styles to different objects (ie one style for a plane, a different style for a bird), this can't be done simultaneously, but can be accomplished with multiple passes and correctly tuned weights, if the objects are segmented on one at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9so8jNAP0G_"
      },
      "source": [
        "background = True #@param {type: \"boolean\"}\n",
        "aeroplane = True #@param {type: \"boolean\"}\n",
        "bicycle = True #@param {type: \"boolean\"}\n",
        "bird = True #@param {type: \"boolean\"}\n",
        "boat = True #@param {type: \"boolean\"}\n",
        "bottle = True #@param {type: \"boolean\"}\n",
        "bus = True #@param {type: \"boolean\"}\n",
        "car = True #@param {type: \"boolean\"}\n",
        "cat = True #@param {type: \"boolean\"}\n",
        "chair = True #@param {type: \"boolean\"}\n",
        "cow = True #@param {type: \"boolean\"}\n",
        "diningtable = True #@param {type: \"boolean\"}\n",
        "dog = True #@param {type: \"boolean\"}\n",
        "horse = True #@param {type: \"boolean\"}\n",
        "motorbike = True #@param {type: \"boolean\"}\n",
        "person = True #@param {type: \"boolean\"}\n",
        "pottedplant = True #@param {type: \"boolean\"}\n",
        "sheep = True #@param {type: \"boolean\"}\n",
        "sofa = True #@param {type: \"boolean\"}\n",
        "train = True #@param {type: \"boolean\"}\n",
        "tvmoniter = True #@param {type: \"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAMmQoRdaZ43"
      },
      "source": [
        "This cell loads and displays the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slxnma0Jz37L"
      },
      "source": [
        "labels = (aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, \n",
        "          diningtable, dog, horse, motorbike, person, pottedplant, sheep, \n",
        "          sofa, train, tvmoniter)\n",
        "\n",
        "shape = (height, width)\n",
        "content_objects = [i + 1 for i, label in enumerate(labels) if label]\n",
        "\n",
        "content_image = transform_image(open_image(content_image_file), shape).to(device)\n",
        "if custom_segment_file is None:\n",
        "  content_segment = segment(content_image, content_objects)\n",
        "else:\n",
        "  content_segment = transform_image(open_image(custom_segment_file), shape).to(device)\n",
        "object_style_image = transform_image(open_image(object_style_image_file), shape).to(device)\n",
        "background_style_image = transform_image(open_image(background_style_image_file), shape).to(device)\n",
        "\n",
        "fig, axarr = plt.subplots(1, 4, figsize=(15, 15))\n",
        "axarr[0].title.set_text(\"Content Image\")\n",
        "axarr[1].title.set_text(\"Segmented Image\")\n",
        "axarr[2].title.set_text(\"Object Style Image\")\n",
        "axarr[3].title.set_text(\"Background Style Image\")\n",
        "for column, image in enumerate((content_image, content_segment, object_style_image, background_style_image)):\n",
        "  axarr[column].imshow(tensor_to_pil(image))\n",
        "  axarr[column].axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-u5HvjinPIl"
      },
      "source": [
        "# Features and Grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e15jAn9onShy"
      },
      "source": [
        "Style transfer requires capturing the \"features\" of an image.  An example of a feature could be any horizontal line in the image.  The features present in an image are found using the pre-trained VGG19 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g1AZJi80nlu"
      },
      "source": [
        "layers = {\n",
        "  \"0\": \"conv1_1\",\n",
        "  \"2\": \"conv1_2\",\n",
        "  \"5\": \"conv2_1\",\n",
        "  \"7\": \"conv2_2\",\n",
        "  \"10\": \"conv3_1\",\n",
        "  \"12\": \"conv3_2\",\n",
        "  \"14\": \"conv3_3\",\n",
        "  \"16\": \"conv3_4\",\n",
        "  \"19\": \"conv4_1\",\n",
        "  \"21\": \"conv4_2\",\n",
        "  \"23\": \"conv4_3\",\n",
        "  \"25\": \"conv4_4\",\n",
        "  \"28\": \"conv5_1\",\n",
        "  \"30\": \"conv5_2\",\n",
        "  \"32\": \"conv5_3\",\n",
        "  \"34\": \"conv5_4\",\n",
        "  }\n",
        "\n",
        "def get_features(image, feature_layers):\n",
        "  features = {}\n",
        "\n",
        "  x = image\n",
        "  for name, layer in vgg._modules.items():\n",
        "    x = layer(x)\n",
        "    if name in layers and layers[name] in feature_layers:\n",
        "      features[layers[name]] = x\n",
        "\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftbV-cnxqGI3"
      },
      "source": [
        "Style representations are obtained by measuring the correlation between different feature map responses of a given layer. The dot product of two vectors can be seen as how similar two vectors are to each other - the more similar they are, the lesser the angle between them. In style transfer, this can be done by flattening the feature map's spatial dimensions at each depth and computing its dot product. The result is the Gram Matrix:\n",
        "\n",
        "$$G_{ij}^l = \\sum_{k}^{} F_{ik}^l F_{jk}^l$$\n",
        "\n",
        "Where $F_{ij}^l$ is the activation of the ith filter at position j in layer l."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FvkAusgCooG"
      },
      "source": [
        "def gram_matrix(tensor):\n",
        "  _, depth, height, width = tensor.size()    # don't need the batch size\n",
        "  tensor = tensor.view(depth, height * width)    # flatten spatial dimensions\n",
        "  gram = torch.mm(tensor, tensor.t())    # matrix multiplication of tensor and its transpose\n",
        "\n",
        "  return gram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4OVIGHjqthL"
      },
      "source": [
        "# Target Image and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvt_HgsqqwNJ"
      },
      "source": [
        "The target image is the result of the style transfer.  It can be composed of random white noise, but better results seem to be obtained by setting it to the content image.  In this case, there are two target images to allow for the two different types of styles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMjxVn7PC9Xa"
      },
      "source": [
        "object_target = content_image.clone().requires_grad_(True).to(device)\n",
        "background_target = content_image.clone().requires_grad_(True).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLQYutIlrRgn"
      },
      "source": [
        "Adam and L-BFGS are the two common types of optimizers used in style transfer.  The Adam optimizer is faster and allows for smaller updates to the target image so that is what's provided here.  To use L-BFGS, all that should be required is changing torch.optim.Adam to torch.optim.LBFGS in the two lines below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqZmulJvFp9G"
      },
      "source": [
        "object_learning_rate = 1e-2 #@param {type: \"number\"}\n",
        "background_learning_rate = 1e-2 #@param {type: \"number\"}\n",
        "\n",
        "object_optimizer = torch.optim.Adam([object_target], lr=object_learning_rate)\n",
        "background_optimizer = torch.optim.Adam([background_target], lr=background_learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD9h0765tytd"
      },
      "source": [
        "# Calculating Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-f1FMjMt4JZ"
      },
      "source": [
        "## Content Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2bQRbNZt6JU"
      },
      "source": [
        "Ensures the activations of higher layers are similar between the content image and the generated image.$${\\cal L}_{content}(\\vec{p},\\vec{x},l) = \\frac{1}{2} \\sum_{i,j}(F_{ij}^l - P_{ij}^l)^2$$Where $\\vec{p}$ and $\\vec{x}$ are the original image and the image that is generated and $P^l$ and $F^l$ their respective feature representation in layer $l$.\n",
        "\n",
        "Convolutional feature maps are generally a good representation of an input image's features. They capture spatial information without containing the style information. Therefore, mean squared difference between the target and content features is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-TtwkgtDD95"
      },
      "source": [
        "def compute_content_loss(image):\n",
        "  image_features = get_features(image, content_layer)\n",
        "  return torch.mean((image_features[content_layer] - content_features[content_layer])**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELaRMQsKuElQ"
      },
      "source": [
        "## Style Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRtF1QPhuHNj"
      },
      "source": [
        "Ensures the correlation of activations in all layers are similar between the style image and the generated image.\n",
        "\n",
        "**Contribution of layer $l$ to total loss**\n",
        "$$E_l = \\frac{1}{4N_l^2M_l^2} \\sum_{i,j}(G_{i,j}^l - A_{i,j}^l)^2$$\n",
        "Where $\\vec{a}$ and $\\vec{x}$ are the original image and the image that is generated and $A^l$ and $G^l$ their respective style representation in layer $l$. $N_l$ is the number of feature maps and $M_l$ is the height * width of the the feature map.\n",
        "\n",
        "**Total style loss**\n",
        "$${\\cal L}_{style}(\\vec{a},\\vec{x}) = \\sum_{l=0}^L w_l E_l$$\n",
        "Where $w_l$ are the weighting factors of the contribution of each layer of the total loss.\n",
        "\n",
        "Style loss is the mean squared difference between the gram matrix of the input and the gram matrix of the style image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-PS3rPdFlya"
      },
      "source": [
        "def compute_style_loss(image, style_features, style_weights, style_grams):\n",
        "  style_loss = 0\n",
        "  image_features = get_features(image, style_weights)\n",
        "\n",
        "  for layer in style_weights:\n",
        "    image_feature = image_features[layer]\n",
        "    image_gram = gram_matrix(image_feature)\n",
        "    _, depth, height, width = image_feature.shape\n",
        "    \n",
        "    style_gram = style_grams[layer]\n",
        "    layer_style_loss = style_weights[layer] * torch.mean((image_gram - style_gram)**2)\n",
        "    \n",
        "    style_loss += layer_style_loss / (depth * height * width)\n",
        "      \n",
        "  return style_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fFpFbWVufGt"
      },
      "source": [
        "## Total Variation Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dor7JRKHujRc"
      },
      "source": [
        "Not mentioned in the original paper is a factor for total variation loss. This measures how much noise is in the images and can be used to smooth the image. There are different calculations for it; in this case, the sum of the means of the absolute differences between adjacent pixels is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoiZoHV5GmzC"
      },
      "source": [
        "def compute_total_variation_loss(image):\n",
        "  total_variation_loss = (torch.mean(torch.abs(image[:,:,:,:-1] - image[:,:,:,1:]))\n",
        "                          + torch.mean(torch.abs(image[:,:,:-1,:] - image[:,:,1:,:])))\n",
        "  \n",
        "  return total_variation_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_LYOA6-vPRN"
      },
      "source": [
        "# Style Transfer Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtDrC76tvXQL"
      },
      "source": [
        "The style transfer loop works by applying style transfer to each of the two target images (one for the objects, one for the background) and keeping a separate image that is a combination of the two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwTTcPgPGsoI"
      },
      "source": [
        "loss_tracker = {\n",
        "    \"content (object)\": 0.0,\n",
        "    \"style (object)\": 0.0,\n",
        "    \"total variation (object)\": 0.0,\n",
        "    \"content (background)\": 0.0,\n",
        "    \"style (background)\": 0.0,\n",
        "    \"total variation (background)\": 0.0,\n",
        "}\n",
        "\n",
        "def style_transfer_loop(total_iterations, display_iterations=0):\n",
        "  for k in loss_tracker.keys():\n",
        "    loss_tracker[k] = 0.0\n",
        "\n",
        "  if display_iterations == 0:\n",
        "    display_iterations = total_iterations\n",
        "      \n",
        "  for iteration in range(1, total_iterations + 1):\n",
        "    def closure(is_object):\n",
        "      global loss_tracker\n",
        "\n",
        "      object_optimizer.zero_grad()\n",
        "      background_optimizer.zero_grad()\n",
        "      \n",
        "      total_loss = 0\n",
        "            \n",
        "      if is_object:\n",
        "        if object_content_weight != 0:\n",
        "          object_content_loss = compute_content_loss(object_target) * object_content_weight\n",
        "          loss_tracker[\"content (object)\"] = object_content_loss\n",
        "          total_loss += object_content_loss\n",
        "\n",
        "        if object_style_weight != 0:\n",
        "          object_style_loss = compute_style_loss(object_target, object_features, object_weights, object_grams) * object_style_weight\n",
        "          loss_tracker[\"style (object)\"] = object_style_loss\n",
        "          total_loss += object_style_loss\n",
        "\n",
        "        if object_total_variation_weight != 0:\n",
        "          object_total_variation_loss = compute_total_variation_loss(object_target) * object_total_variation_weight\n",
        "          loss_tracker[\"total variation (object)\"] = object_total_variation_loss\n",
        "          total_loss += object_total_variation_loss\n",
        "\n",
        "      else:\n",
        "        if background_content_weight != 0:\n",
        "          background_content_loss = compute_content_loss(background_target) * background_content_weight\n",
        "          loss_tracker[\"content (background)\"] = background_content_loss\n",
        "          total_loss += background_content_loss\n",
        "\n",
        "        if background_style_weight != 0:\n",
        "          background_style_loss = compute_style_loss(background_target, background_features, background_weights, background_grams) * background_style_weight\n",
        "          loss_tracker[\"style (background)\"] = background_style_loss\n",
        "          total_loss += background_style_loss\n",
        "\n",
        "        if background_total_variation_weight != 0:\n",
        "          background_total_variation_loss = compute_total_variation_loss(background_target) * background_total_variation_weight\n",
        "          loss_tracker[\"total variation (background)\"] = background_total_variation_loss\n",
        "          total_loss += background_total_variation_loss\n",
        "            \n",
        "      total_loss.backward()\n",
        "\n",
        "      return total_loss\n",
        "        \n",
        "    def object_closure():\n",
        "      return closure(True)\n",
        "\n",
        "    def background_closure():\n",
        "      return closure(False)\n",
        "\n",
        "    if object_content_weight or object_style_weight or object_total_variation_weight:\n",
        "      object_optimizer.step(object_closure)\n",
        "    if background_content_weight or background_style_weight or background_total_variation_weight:\n",
        "      background_optimizer.step(background_closure)\n",
        "  \n",
        "    transfer_object = convert_to_original_colors(tensor_to_pil(object_target)) if is_converting_object_colors else object_target\n",
        "    transfer_background = convert_to_original_colors(tensor_to_pil(background_target)) if is_converting_background_colors else background_target\n",
        "    transfer_image = tensor_to_pil(update_final_image(transfer_object, transfer_background))\n",
        "\n",
        "    if iteration % display_iterations == 0 or iteration == total_iterations:\n",
        "      print(\"Iteration: \", iteration)\n",
        "      print(\"Content (object): {}, Style (object): {}, TV (object): {},\\nContent (background): {}, Style (background): {}, TV (background): {}\"\n",
        "        .format(loss_tracker[\"content (object)\"],\n",
        "                loss_tracker[\"style (object)\"],\n",
        "                loss_tracker[\"total variation (object)\"],\n",
        "                loss_tracker[\"content (background)\"],\n",
        "                loss_tracker[\"style (background)\"],\n",
        "                loss_tracker[\"total variation (background)\"]\n",
        "                ))\n",
        "      plt.imshow(transfer_image)\n",
        "      plt.axis(\"off\")\n",
        "      plt.show()\n",
        "\n",
        "  return transfer_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iu43_jbu4Ym"
      },
      "source": [
        "# Weight Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUVeOyXUu6h-"
      },
      "source": [
        "In the logical flow of the notebook, these should be higher up.  They were left until the end to simplify updating them before running the style transfer loop when making multiple passes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiYQxp8Tw7-j"
      },
      "source": [
        "When applying style transfer to an entire image or to only the objects of an image, the background weights should all be set to 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4KnhvL5ooXe"
      },
      "source": [
        "#@markdown Object Weights\n",
        "object_content_weight = 1e3 #@param {type: \"number\"}\n",
        "object_style_weight = 1e2 #@param {type: \"number\"}\n",
        "object_total_variation_weight = 1e-1 #@param {type: \"number\"}\n",
        "\n",
        "#@markdown Background Weights\n",
        "background_content_weight =  0#@param {type: \"number\"}\n",
        "background_style_weight =  0#@param {type: \"number\"}\n",
        "background_total_variation_weight =  0#@param {type: \"number\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxKAUWZqxMKg"
      },
      "source": [
        "Which layer of VGG will be used to represent the content image's features.  The names of all possible layers that can be included can be found in the Features and Grams section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy4WeHeOCrN2"
      },
      "source": [
        "content_layer = \"conv4_2\"  #@param {type: \"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFJQTTrGwQ51"
      },
      "source": [
        "These are meant to be tuneable parameters.  Using Colab's parameter settings makes a mess of things, so they're left as a code cell, but they are meant to adjusted.  They determine how much each layer contributes to the style of the image.\n",
        "\n",
        "Weighting earlier layers more (eg conv1_1) results in larger style artifacts in the generated image. Weighting later layers more emphasizes smaller features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0avv7mbwoiU8"
      },
      "source": [
        "object_weights = {\n",
        "    \"conv1_1\": 1e4,\n",
        "    \"conv2_1\": 1e3,\n",
        "    \"conv3_1\": 1e2,\n",
        "    \"conv4_1\": 1e1,\n",
        "    \"conv5_1\": 1e1,\n",
        "}\n",
        "\n",
        "background_weights = {\n",
        "    \"conv1_1\": 1e4,\n",
        "    \"conv2_1\": 1e3,\n",
        "    \"conv3_1\": 1e2,\n",
        "    \"conv4_1\": 1e1,\n",
        "    \"conv5_1\": 1e1,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPA18h1oyL-K"
      },
      "source": [
        "It may be preferable to keep the colors consistent with the original content image.  These can be toggled to do so for the objects and/or the background of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYQ_0vnlsKzC"
      },
      "source": [
        "is_converting_object_colors = False #@param {type: \"boolean\"}\n",
        "is_converting_background_colors = False #@param {type: \"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pku3aVDkymqI"
      },
      "source": [
        "# Running"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZUgNqptyr_o"
      },
      "source": [
        "Finally, this is where the actual transfer happens.  The two parameters are how many times to run through the style transfer loop and on which of those iterations the current image should be displayed.  The first cell only needs to be run once unless something above it has been changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nHB9LgoiUVt"
      },
      "source": [
        "content_features = get_features(content_image, content_layer)\n",
        "object_features = get_features(object_style_image, object_weights)\n",
        "object_grams = {layer: gram_matrix(object_features[layer]) for layer in object_features}\n",
        "background_features = get_features(background_style_image, background_weights)\n",
        "background_grams = {layer: gram_matrix(background_features[layer]) for layer in background_features}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14rQ5mcwzE32"
      },
      "source": [
        "total_iterations = 500 #@param {type: \"number\"}\n",
        "display_iterations = 20 #@param {type: \"number\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeqBx57lKxA4"
      },
      "source": [
        "final_image = style_transfer_loop(total_iterations, display_iterations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJe8EM4MzTZ6"
      },
      "source": [
        "# Saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyHLpT-fVldy"
      },
      "source": [
        "save_file_name = \"generated_image.png\"\n",
        "final_image.save(save_file_name)\n",
        "files.download(save_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}